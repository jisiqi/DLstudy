{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "F:\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "F:\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "F:\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "F:\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "F:\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "F:\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "F:\\Anaconda3\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "F:\\Anaconda3\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "F:\\Anaconda3\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "F:\\Anaconda3\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "F:\\Anaconda3\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "F:\\Anaconda3\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    }
   ],
   "source": [
    "import keras\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from keras.models import Model\n",
    "from keras.layers.core import Dense, Dropout, Activation, Flatten\n",
    "from keras.models import Sequential\n",
    "from keras.utils import np_utils\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import r2_score\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Model\n",
    "from keras.losses import mse\n",
    "from keras.layers import Concatenate, Dense, LSTM, Input, concatenate,Conv2D,MaxPooling2D,BatchNormalization,UpSampling2D,Lambda,Reshape\n",
    "from keras import backend as K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def imshow(image, cm='hot') :\n",
    "    sns.set()\n",
    "    sns.heatmap(image, cmap=cm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From F:\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:517: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tf_x = K.placeholder(dtype=tf.float32,shape=(None, 200, 200, 4))\n",
    "tf_y = K.placeholder(dtype=tf.float32,shape=(None, 200, 200, 1))\n",
    "tf_z = K.placeholder(dtype=tf.float32,shape=(None, 7))\n",
    "is_t = K.placeholder(dtype=tf.bool,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 设定随机数\n",
    "seed = 7\n",
    "np.random.seed(seed)\n",
    "# 类别个数\n",
    "num_classes = None\n",
    "# 图片大小\n",
    "lng = 200\n",
    "# 属性维度\n",
    "att_d = 4\n",
    "int_shp = (lng, lng, att_d)\n",
    "cut_num = 300"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading train_x\n",
      "loading train_y\n",
      "loading train_z\n",
      "loading pred_x\n",
      "loading pred_y\n",
      "loading pred_z\n"
     ]
    }
   ],
   "source": [
    "pred_x_name = 'E:/20190924_match_pro/MHNN/data/scale_data/pred_x2.npy'\n",
    "pred_y_name = 'E:/20190924_match_pro/MHNN/data/scale_data/pred_y2.npy'\n",
    "pred_z_name = 'E:/20190924_match_pro/MHNN/data/scale_data/pred_z2.npy'\n",
    "train_x_name= 'E:/20190924_match_pro/MHNN/data/scale_data/train_x2.npy'\n",
    "train_y_name= 'E:/20190924_match_pro/MHNN/data/scale_data/train_y2.npy'\n",
    "train_z_name= 'E:/20190924_match_pro/MHNN/data/scale_data/train_z2.npy'\n",
    "\n",
    "print('loading train_x')\n",
    "train_x = np.load(train_x_name)\n",
    "print('loading train_y')\n",
    "train_y = np.load(train_y_name)\n",
    "print('loading train_z')\n",
    "train_z = np.load(train_z_name)\n",
    "print('loading pred_x')\n",
    "pred_x  = np.load(pred_x_name)\n",
    "print('loading pred_y')\n",
    "pred_y  = np.load(pred_y_name)\n",
    "print('loading pred_z')\n",
    "pred_z  = np.load(pred_z_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(300, 200, 200, 4)\n",
      "(300, 200, 200, 1)\n",
      "(300, 7)\n",
      "(1300, 200, 200, 4)\n"
     ]
    }
   ],
   "source": [
    "print(train_x.shape)\n",
    "print(train_y.shape)\n",
    "print(train_z.shape)\n",
    "\n",
    "print(pred_x.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "##定义模型\n",
    "def MHNN_model():    \n",
    "  \n",
    "    main_input = Input(shape = (200,200,4), name='main_input')\n",
    "    \n",
    "    conv1 = Conv2D(filters=16, strides=1, kernel_size=3,activation='relu', padding='same')(main_input)\n",
    "    conv1 = Conv2D(filters=16, strides=1, kernel_size=3,activation='relu', padding='same')(conv1)\n",
    "    conv1 = Conv2D(filters=16, strides=1, kernel_size=3,activation='relu', padding='same')(conv1)\n",
    "    pool1 = MaxPooling2D(pool_size=(2, 2))(conv1)\n",
    "    BN1   = BatchNormalization()(pool1)\n",
    "    \n",
    "    conv2 = Conv2D(filters=64, strides=1, kernel_size=3,activation='relu', padding='same')(BN1)\n",
    "    conv2 = Conv2D(filters=64, strides=1, kernel_size=3,activation='relu', padding='same')(conv2)\n",
    "    conv2 = Conv2D(filters=64, strides=1, kernel_size=3,activation='relu', padding='same')(conv2)\n",
    "    pool2 = MaxPooling2D(pool_size=(2, 2))(conv2)\n",
    "    BN2   = BatchNormalization()(pool2)\n",
    "    \n",
    "    conv3 = Conv2D(filters=64, strides=1, kernel_size=3,activation='relu', padding='same')(BN2)\n",
    "    conv3 = Conv2D(filters=64, strides=1, kernel_size=3,activation='relu', padding='same')(conv3)\n",
    "    conv3 = Conv2D(filters=64, strides=1, kernel_size=3,activation='relu', padding='same')(conv3)\n",
    "    BN3   = BatchNormalization()(conv3)\n",
    "    \n",
    "    up1   = UpSampling2D(size = (2,2))(BN3)\n",
    "    up1   = Conv2D(filters=64, strides=1, kernel_size=3,activation='relu', padding='same')(up1)\n",
    "    \n",
    "    merge1 = concatenate([conv2,up1], axis = 3)\n",
    "    \n",
    "    conv4 = Conv2D(filters=64, strides=1, kernel_size=3,activation='relu', padding='same')(merge1)\n",
    "    conv4 = Conv2D(filters=64, strides=1, kernel_size=3,activation='relu', padding='same')(conv4)\n",
    "    conv4 = Conv2D(filters=64, strides=1, kernel_size=3,activation='relu', padding='same')(conv4)\n",
    "    \n",
    "    up2   = UpSampling2D(size = (2,2))(conv4)\n",
    "    up2   = Conv2D(filters=16, strides=1, kernel_size=3,activation='relu', padding='same')(up2)\n",
    "    \n",
    "    conv5 = Conv2D(filters=16, strides=1, kernel_size=3,activation='relu', padding='same')(up2)\n",
    "    conv5 = Conv2D(filters=16, strides=1, kernel_size=3,activation='relu', padding='same')(conv5)\n",
    "    conv5 = Conv2D(filters=16, strides=1, kernel_size=3,activation='relu', padding='same')(conv5)\n",
    "    conv5 = Conv2D(filters=4, strides=1, kernel_size=3,activation='relu', padding='same')(conv5)\n",
    "    \n",
    "    output1 = Conv2D(filters=1, strides=1, kernel_size=3, activation = 'sigmoid', padding = 'same')(conv5)\n",
    "    \n",
    "    output1_ = tf.reshape(output1, [-1, conv5.shape[1]*conv5.shape[2]])\n",
    "    tf_y_    = tf.reshape(tf_y, [-1, conv5.shape[1]*conv5.shape[2]])\n",
    "\n",
    "    \n",
    "#     tf_x       = Conv2D(filters=4, strides=1, kernel_size=3,activation='relu', padding='same')(tf_x_input)\n",
    "    tf_x   = Input(shape = (200,200,1))\n",
    "    merge2 = concatenate([tf_x,output1], axis = 3)\n",
    "#     merge2 = Lambda(lambda x: concatenate([tf_x,output1], axis = 3))\n",
    "    \n",
    "    conv6 = Conv2D(filters=16, strides=1, kernel_size=3,activation='relu', padding='same')(merge2)\n",
    "    conv6 = Conv2D(filters=16, strides=1, kernel_size=3,activation='relu', padding='same')(conv6)\n",
    "    conv6 = Conv2D(filters=16, strides=1, kernel_size=3,activation='relu', padding='same')(conv6)\n",
    "    pool6 = MaxPooling2D(pool_size=(2, 2))(conv6)\n",
    "    BN6   = BatchNormalization()(pool6)\n",
    "    \n",
    "    merge3 = concatenate([pool6, pool1, conv4], 3) # [pool5, pool1, conv4]\n",
    "    \n",
    "    conv7 = Conv2D(filters=16, strides=1, kernel_size=3,activation='relu', padding='same')(merge3)\n",
    "    conv7 = Conv2D(filters=16, strides=1, kernel_size=3,activation='relu', padding='same')(conv7)\n",
    "    conv7 = Conv2D(filters=16, strides=1, kernel_size=3,activation='relu', padding='same')(conv7)\n",
    "    pool7 = MaxPooling2D(pool_size=(2, 2))(conv7)\n",
    "    \n",
    "    merge4 = concatenate([pool7, pool2, conv3], 3)\n",
    "    \n",
    "    conv8 = Conv2D(filters=16, strides=1, kernel_size=3,activation='relu', padding='same')(merge4)\n",
    "    conv8 = Conv2D(filters=16, strides=1, kernel_size=3,activation='relu', padding='same')(conv8)\n",
    "    conv8 = Conv2D(filters=16, strides=1, kernel_size=3,activation='relu', padding='same')(conv8)\n",
    "    pool8 = MaxPooling2D(pool_size=(2, 2))(conv8)\n",
    "    \n",
    "    \n",
    "#     dense8 = Lambda(lambda x: tf.reshape(pool8, [-1, 25*25*16]))\n",
    "#     input3 = Input()\n",
    "\n",
    "    \n",
    "#     dense8 = Reshape((-1,25*25*16))(pool8)\n",
    "    \n",
    "    dense8 = Dense(2048, activation='relu')(pool8)\n",
    "    BN8    = BatchNormalization()(dense8)\n",
    "    \n",
    "    dense9 = Dense(256, activation='relu')(BN8)\n",
    "    BN9    = BatchNormalization()(dense9)\n",
    "    \n",
    "#     output2 = Dense(7)(BN9)\n",
    "    output2 = Reshape((200,200,4))(BN9)\n",
    "    model = Model(inputs=[main_input,tf_x], outputs=[output2])\n",
    "    \n",
    "    \n",
    "    \n",
    "#     def MHNN_loss_function(y_true, y_pred):\n",
    "        \n",
    "#         c = BinaryCrossentropy(tf_y_, output1_)\n",
    "#         r = mse(output2, tf_z)\n",
    "#         vc = tf.cast(c,dtype=tf.float32)\n",
    "#         vr = tf.cast(a,dtype=tf.float32)\n",
    "        \n",
    "#         loss_cr = vr * 0.1 + vc * 0.9 \n",
    "#         return loss_cr\n",
    "        \n",
    "        \n",
    "    \n",
    "    \n",
    "    \n",
    "    model.compile(loss='mse', metrics=['accuracy'], optimizer='rmsprop')\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From F:\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:74: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n",
      "WARNING:tensorflow:From F:\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:4138: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
      "\n",
      "WARNING:tensorflow:From F:\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:3976: The name tf.nn.max_pool is deprecated. Please use tf.nn.max_pool2d instead.\n",
      "\n",
      "WARNING:tensorflow:From F:\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:174: The name tf.get_default_session is deprecated. Please use tf.compat.v1.get_default_session instead.\n",
      "\n",
      "WARNING:tensorflow:From F:\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:181: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.\n",
      "\n",
      "WARNING:tensorflow:From F:\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:1834: The name tf.nn.fused_batch_norm is deprecated. Please use tf.compat.v1.nn.fused_batch_norm instead.\n",
      "\n",
      "WARNING:tensorflow:From F:\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:2018: The name tf.image.resize_nearest_neighbor is deprecated. Please use tf.compat.v1.image.resize_nearest_neighbor instead.\n",
      "\n",
      "WARNING:tensorflow:From F:\\Anaconda3\\lib\\site-packages\\keras\\optimizers.py:790: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "main_input (InputLayer)         (None, 200, 200, 4)  0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_1 (Conv2D)               (None, 200, 200, 16) 592         main_input[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_2 (Conv2D)               (None, 200, 200, 16) 2320        conv2d_1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_3 (Conv2D)               (None, 200, 200, 16) 2320        conv2d_2[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2D)  (None, 100, 100, 16) 0           conv2d_3[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_1 (BatchNor (None, 100, 100, 16) 64          max_pooling2d_1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_4 (Conv2D)               (None, 100, 100, 64) 9280        batch_normalization_1[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_5 (Conv2D)               (None, 100, 100, 64) 36928       conv2d_4[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_6 (Conv2D)               (None, 100, 100, 64) 36928       conv2d_5[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2D)  (None, 50, 50, 64)   0           conv2d_6[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_2 (BatchNor (None, 50, 50, 64)   256         max_pooling2d_2[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_7 (Conv2D)               (None, 50, 50, 64)   36928       batch_normalization_2[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_8 (Conv2D)               (None, 50, 50, 64)   36928       conv2d_7[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_9 (Conv2D)               (None, 50, 50, 64)   36928       conv2d_8[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_3 (BatchNor (None, 50, 50, 64)   256         conv2d_9[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "up_sampling2d_1 (UpSampling2D)  (None, 100, 100, 64) 0           batch_normalization_3[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_10 (Conv2D)              (None, 100, 100, 64) 36928       up_sampling2d_1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_1 (Concatenate)     (None, 100, 100, 128 0           conv2d_6[0][0]                   \n",
      "                                                                 conv2d_10[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_11 (Conv2D)              (None, 100, 100, 64) 73792       concatenate_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_12 (Conv2D)              (None, 100, 100, 64) 36928       conv2d_11[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_13 (Conv2D)              (None, 100, 100, 64) 36928       conv2d_12[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "up_sampling2d_2 (UpSampling2D)  (None, 200, 200, 64) 0           conv2d_13[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_14 (Conv2D)              (None, 200, 200, 16) 9232        up_sampling2d_2[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_15 (Conv2D)              (None, 200, 200, 16) 2320        conv2d_14[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_16 (Conv2D)              (None, 200, 200, 16) 2320        conv2d_15[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_17 (Conv2D)              (None, 200, 200, 16) 2320        conv2d_16[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_18 (Conv2D)              (None, 200, 200, 4)  580         conv2d_17[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "input_1 (InputLayer)            (None, 200, 200, 1)  0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_19 (Conv2D)              (None, 200, 200, 1)  37          conv2d_18[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_2 (Concatenate)     (None, 200, 200, 2)  0           input_1[0][0]                    \n",
      "                                                                 conv2d_19[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_20 (Conv2D)              (None, 200, 200, 16) 304         concatenate_2[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_21 (Conv2D)              (None, 200, 200, 16) 2320        conv2d_20[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_22 (Conv2D)              (None, 200, 200, 16) 2320        conv2d_21[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_3 (MaxPooling2D)  (None, 100, 100, 16) 0           conv2d_22[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_3 (Concatenate)     (None, 100, 100, 96) 0           max_pooling2d_3[0][0]            \n",
      "                                                                 max_pooling2d_1[0][0]            \n",
      "                                                                 conv2d_13[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_23 (Conv2D)              (None, 100, 100, 16) 13840       concatenate_3[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_24 (Conv2D)              (None, 100, 100, 16) 2320        conv2d_23[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_25 (Conv2D)              (None, 100, 100, 16) 2320        conv2d_24[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_4 (MaxPooling2D)  (None, 50, 50, 16)   0           conv2d_25[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_4 (Concatenate)     (None, 50, 50, 144)  0           max_pooling2d_4[0][0]            \n",
      "                                                                 max_pooling2d_2[0][0]            \n",
      "                                                                 conv2d_9[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_26 (Conv2D)              (None, 50, 50, 16)   20752       concatenate_4[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_27 (Conv2D)              (None, 50, 50, 16)   2320        conv2d_26[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_28 (Conv2D)              (None, 50, 50, 16)   2320        conv2d_27[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_5 (MaxPooling2D)  (None, 25, 25, 16)   0           conv2d_28[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 25, 25, 2048) 34816       max_pooling2d_5[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_5 (BatchNor (None, 25, 25, 2048) 8192        dense_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_2 (Dense)                 (None, 25, 25, 256)  524544      batch_normalization_5[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_6 (BatchNor (None, 25, 25, 256)  1024        dense_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "reshape_1 (Reshape)             (None, 200, 200, 4)  0           batch_normalization_6[0][0]      \n",
      "==================================================================================================\n",
      "Total params: 1,018,505\n",
      "Trainable params: 1,013,609\n",
      "Non-trainable params: 4,896\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = MHNN_model()\n",
    "model.summary()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 定义损失函数以及准确率回收\n",
    "class Loss_History(keras.callbacks.Callback):\n",
    "    def on_train_begin(self, logs={}):\n",
    "        self.losses = {'batch': [], 'epoch': []}\n",
    "    def on_batch_end(self, batch, logs={}):\n",
    "        self.losses['batch'].append(logs.get('loss'))\n",
    "    def on_epoch_end(self, batch, logs={}):\n",
    "        self.losses['epoch'].append(logs.get('loss'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n"
     ]
    },
    {
     "ename": "AbortedError",
     "evalue": "Operation received an exception:Status: 5, message: could not create a view primitive descriptor, in file tensorflow/core/kernels/mkl_slice_op.cc:433\n\t [[{{node training/RMSprop/gradients/concatenate_2/concat_grad/Slice_1}}]]",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAbortedError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-12-25bae6a4ed97>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# 调整\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0mhistory\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mLoss_History\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mtrain_x\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mtrain_y\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpred_x\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;36m300\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m50\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m5\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mshuffle\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;31m# print('saving model')\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mF:\\Anaconda3\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[0;32m   1037\u001b[0m                                         \u001b[0minitial_epoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1038\u001b[0m                                         \u001b[0msteps_per_epoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1039\u001b[1;33m                                         validation_steps=validation_steps)\n\u001b[0m\u001b[0;32m   1040\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1041\u001b[0m     def evaluate(self, x=None, y=None,\n",
      "\u001b[1;32mF:\\Anaconda3\\lib\\site-packages\\keras\\engine\\training_arrays.py\u001b[0m in \u001b[0;36mfit_loop\u001b[1;34m(model, f, ins, out_labels, batch_size, epochs, verbose, callbacks, val_f, val_ins, shuffle, callback_metrics, initial_epoch, steps_per_epoch, validation_steps)\u001b[0m\n\u001b[0;32m    197\u001b[0m                     \u001b[0mins_batch\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mins_batch\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    198\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 199\u001b[1;33m                 \u001b[0mouts\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    200\u001b[0m                 \u001b[0mouts\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mto_list\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    201\u001b[0m                 \u001b[1;32mfor\u001b[0m \u001b[0ml\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mo\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mout_labels\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mouts\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mF:\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m   2713\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_legacy_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2714\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2715\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2716\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2717\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mpy_any\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mis_tensor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mF:\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py\u001b[0m in \u001b[0;36m_call\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m   2673\u001b[0m             \u001b[0mfetched\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun_metadata\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2674\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2675\u001b[1;33m             \u001b[0mfetched\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2676\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mfetched\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2677\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mF:\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1456\u001b[0m         ret = tf_session.TF_SessionRunCallable(self._session._session,\n\u001b[0;32m   1457\u001b[0m                                                \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_handle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1458\u001b[1;33m                                                run_metadata_ptr)\n\u001b[0m\u001b[0;32m   1459\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1460\u001b[0m           \u001b[0mproto_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAbortedError\u001b[0m: Operation received an exception:Status: 5, message: could not create a view primitive descriptor, in file tensorflow/core/kernels/mkl_slice_op.cc:433\n\t [[{{node training/RMSprop/gradients/concatenate_2/concat_grad/Slice_1}}]]"
     ]
    }
   ],
   "source": [
    "# 调整\n",
    "history = Loss_History()\n",
    "model.fit([train_x,train_y], pred_x[0:300,:], epochs=50, batch_size=5, shuffle=False)\n",
    "\n",
    "# print('saving model')\n",
    "# model.save('E:/20190924_match_pro/classification/model/20190402_vgg_3c_10e_116.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
