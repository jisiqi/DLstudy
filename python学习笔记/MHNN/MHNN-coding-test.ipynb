{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "F:\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "F:\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "F:\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "F:\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "F:\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "F:\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "F:\\Anaconda3\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "F:\\Anaconda3\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "F:\\Anaconda3\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "F:\\Anaconda3\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "F:\\Anaconda3\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "F:\\Anaconda3\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    }
   ],
   "source": [
    "import keras\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from keras.models import Model\n",
    "from keras.layers.core import Dense, Dropout, Activation, Flatten\n",
    "from keras.models import Sequential\n",
    "from keras.utils import np_utils\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import r2_score\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Model\n",
    "from keras.layers import Concatenate, Dense, Input, concatenate,Conv2D,MaxPooling2D,BatchNormalization,UpSampling2D,Lambda,Reshape\n",
    "from keras import backend as K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def imshow(image, cm='hot') :\n",
    "    sns.set()\n",
    "    sns.heatmap(image, cmap=cm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_x = K.placeholder(dtype=tf.float32,shape=(None, 200, 200, 4))\n",
    "tf_y = K.placeholder(dtype=tf.float32,shape=(None, 200, 200, 1))\n",
    "tf_z = K.placeholder(dtype=tf.float32,shape=(None, 7))\n",
    "is_t = K.placeholder(dtype=tf.bool,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 设定随机数\n",
    "seed = 7\n",
    "np.random.seed(seed)\n",
    "# 类别个数\n",
    "num_classes = None\n",
    "# 图片大小\n",
    "lng = 200\n",
    "# 属性维度\n",
    "att_d = 4\n",
    "int_shp = (lng, lng, att_d)\n",
    "cut_num = 300"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading train_x\n",
      "loading train_y\n",
      "loading train_z\n",
      "loading pred_x\n",
      "loading pred_y\n",
      "loading pred_z\n"
     ]
    }
   ],
   "source": [
    "pred_x_name = 'E:/20190924_match_pro/MHNN/data/scale_data/pred_x2.npy'\n",
    "pred_y_name = 'E:/20190924_match_pro/MHNN/data/scale_data/pred_y2.npy'\n",
    "pred_z_name = 'E:/20190924_match_pro/MHNN/data/scale_data/pred_z2.npy'\n",
    "train_x_name= 'E:/20190924_match_pro/MHNN/data/scale_data/train_x2.npy'\n",
    "train_y_name= 'E:/20190924_match_pro/MHNN/data/scale_data/train_y2.npy'\n",
    "train_z_name= 'E:/20190924_match_pro/MHNN/data/scale_data/train_z2.npy'\n",
    "\n",
    "print('loading train_x')\n",
    "train_x = np.load(train_x_name)\n",
    "print('loading train_y')\n",
    "train_y = np.load(train_y_name)\n",
    "print('loading train_z')\n",
    "train_z = np.load(train_z_name)\n",
    "print('loading pred_x')\n",
    "pred_x  = np.load(pred_x_name)\n",
    "print('loading pred_y')\n",
    "pred_y  = np.load(pred_y_name)\n",
    "print('loading pred_z')\n",
    "pred_z  = np.load(pred_z_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "#定义模型\n",
    "def MHNN_model():    \n",
    "  \n",
    "    main_input = Input(shape = (200,200,4), name='main_input')\n",
    "    \n",
    "    conv1 = Conv2D(filters=16, strides=1, kernel_size=3,activation='relu', padding='same')(main_input)\n",
    "    conv1 = Conv2D(filters=16, strides=1, kernel_size=3,activation='relu', padding='same')(conv1)\n",
    "    conv1 = Conv2D(filters=16, strides=1, kernel_size=3,activation='relu', padding='same')(conv1)\n",
    "    pool1 = MaxPooling2D(pool_size=(2, 2))(conv1)\n",
    "    BN1   = BatchNormalization()(pool1)\n",
    "    \n",
    "    conv2 = Conv2D(filters=64, strides=1, kernel_size=3,activation='relu', padding='same')(BN1)\n",
    "    conv2 = Conv2D(filters=64, strides=1, kernel_size=3,activation='relu', padding='same')(conv2)\n",
    "    conv2 = Conv2D(filters=64, strides=1, kernel_size=3,activation='relu', padding='same')(conv2)\n",
    "    pool2 = MaxPooling2D(pool_size=(2, 2))(conv2)\n",
    "    BN2   = BatchNormalization()(pool2)\n",
    "    \n",
    "    conv3 = Conv2D(filters=64, strides=1, kernel_size=3,activation='relu', padding='same')(BN2)\n",
    "    conv3 = Conv2D(filters=64, strides=1, kernel_size=3,activation='relu', padding='same')(conv3)\n",
    "    conv3 = Conv2D(filters=64, strides=1, kernel_size=3,activation='relu', padding='same')(conv3)\n",
    "    BN3   = BatchNormalization()(conv3)\n",
    "    \n",
    "    up1   = UpSampling2D(size = (2,2))(BN3)\n",
    "    up1   = Conv2D(filters=64, strides=1, kernel_size=3,activation='relu', padding='same')(up1)\n",
    "    \n",
    "    merge1 = concatenate([conv2,up1], axis = 3)\n",
    "    \n",
    "    conv4 = Conv2D(filters=64, strides=1, kernel_size=3,activation='relu', padding='same')(merge1)\n",
    "    conv4 = Conv2D(filters=64, strides=1, kernel_size=3,activation='relu', padding='same')(conv4)\n",
    "    conv4 = Conv2D(filters=64, strides=1, kernel_size=3,activation='relu', padding='same')(conv4)\n",
    "    \n",
    "    up2   = UpSampling2D(size = (2,2))(conv4)\n",
    "    up2   = Conv2D(filters=16, strides=1, kernel_size=3,activation='relu', padding='same')(up2)\n",
    "    \n",
    "    conv5 = Conv2D(filters=16, strides=1, kernel_size=3,activation='relu', padding='same')(up2)\n",
    "    conv5 = Conv2D(filters=16, strides=1, kernel_size=3,activation='relu', padding='same')(conv5)\n",
    "    conv5 = Conv2D(filters=16, strides=1, kernel_size=3,activation='relu', padding='same')(conv5)\n",
    "    conv5 = Conv2D(filters=4, strides=1, kernel_size=3,activation='relu', padding='same')(conv5)\n",
    "    \n",
    "    output1 = Conv2D(filters=1, strides=1, kernel_size=3, activation = 'sigmoid', padding = 'same')(conv5)\n",
    "    \n",
    "    output1_ = tf.reshape(output1, [-1, conv5.shape[1]*conv5.shape[2]])\n",
    "    tf_y_    = tf.reshape(tf_y, [-1, conv5.shape[1]*conv5.shape[2]])\n",
    "\n",
    "    \n",
    "#     tf_x       = Conv2D(filters=4, strides=1, kernel_size=3,activation='relu', padding='same')(tf_x_input)\n",
    "    tf_x   = Input(shape = (200,200,4))\n",
    "    merge2 = concatenate([tf_x,output1], axis = 3)\n",
    "#     merge2 = Lambda(lambda x: concatenate([tf_x,output1], axis = 3))\n",
    "    \n",
    "    conv6 = Conv2D(filters=16, strides=1, kernel_size=3,activation='relu', padding='same')(merge2)\n",
    "    conv6 = Conv2D(filters=16, strides=1, kernel_size=3,activation='relu', padding='same')(conv6)\n",
    "    conv6 = Conv2D(filters=16, strides=1, kernel_size=3,activation='relu', padding='same')(conv6)\n",
    "    pool6 = MaxPooling2D(pool_size=(2, 2))(conv6)\n",
    "    BN6   = BatchNormalization()(pool6)\n",
    "    \n",
    "    merge3 = concatenate([pool6, pool1, conv4], 3) # [pool5, pool1, conv4]\n",
    "    \n",
    "    conv7 = Conv2D(filters=16, strides=1, kernel_size=3,activation='relu', padding='same')(merge3)\n",
    "    conv7 = Conv2D(filters=16, strides=1, kernel_size=3,activation='relu', padding='same')(conv7)\n",
    "    conv7 = Conv2D(filters=16, strides=1, kernel_size=3,activation='relu', padding='same')(conv7)\n",
    "    pool7 = MaxPooling2D(pool_size=(2, 2))(conv7)\n",
    "    \n",
    "    merge4 = concatenate([pool7, pool2, conv3], 3)\n",
    "    \n",
    "    conv8 = Conv2D(filters=16, strides=1, kernel_size=3,activation='relu', padding='same')(merge4)\n",
    "    conv8 = Conv2D(filters=16, strides=1, kernel_size=3,activation='relu', padding='same')(conv8)\n",
    "    conv8 = Conv2D(filters=16, strides=1, kernel_size=3,activation='relu', padding='same')(conv8)\n",
    "    pool8 = MaxPooling2D(pool_size=(2, 2))(conv8)\n",
    "    \n",
    "    \n",
    "#     dense8 = Lambda(lambda x: tf.reshape(pool8, [-1, 25*25*16]))\n",
    "#     input3 = Input()\n",
    "\n",
    "    \n",
    "    dense8 = Reshape((-1,25*25*16))(pool8)\n",
    "    \n",
    "    dense8 = Dense(2048, activation='relu')(dense8)\n",
    "    BN8    = BatchNormalization()(dense8)\n",
    "    \n",
    "    dense9 = Dense(256, activation='relu')(BN8)\n",
    "    BN9    = BatchNormalization()(dense9)\n",
    "    \n",
    "    output2 = Dense(7)(BN9)\n",
    "    \n",
    "    \n",
    "    model = Model(inputs=[main_input,tf_x], outputs=[output2])\n",
    "    \n",
    "    \n",
    "    \n",
    "#     def MHNN_loss_function(y_true, y_pred):\n",
    "        \n",
    "        \n",
    "    \n",
    "    \n",
    "    \n",
    "    model.compile(loss='mse', metrics=['accuracy'], optimizer='rmsprop')\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_5\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "main_input (InputLayer)         (None, 200, 200, 4)  0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_469 (Conv2D)             (None, 200, 200, 16) 592         main_input[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_470 (Conv2D)             (None, 200, 200, 16) 2320        conv2d_469[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_471 (Conv2D)             (None, 200, 200, 16) 2320        conv2d_470[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_83 (MaxPooling2D) (None, 100, 100, 16) 0           conv2d_471[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_76 (BatchNo (None, 100, 100, 16) 64          max_pooling2d_83[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_472 (Conv2D)             (None, 100, 100, 64) 9280        batch_normalization_76[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_473 (Conv2D)             (None, 100, 100, 64) 36928       conv2d_472[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_474 (Conv2D)             (None, 100, 100, 64) 36928       conv2d_473[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_84 (MaxPooling2D) (None, 50, 50, 64)   0           conv2d_474[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_77 (BatchNo (None, 50, 50, 64)   256         max_pooling2d_84[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_475 (Conv2D)             (None, 50, 50, 64)   36928       batch_normalization_77[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_476 (Conv2D)             (None, 50, 50, 64)   36928       conv2d_475[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_477 (Conv2D)             (None, 50, 50, 64)   36928       conv2d_476[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_78 (BatchNo (None, 50, 50, 64)   256         conv2d_477[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "up_sampling2d_35 (UpSampling2D) (None, 100, 100, 64) 0           batch_normalization_78[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_478 (Conv2D)             (None, 100, 100, 64) 36928       up_sampling2d_35[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_66 (Concatenate)    (None, 100, 100, 128 0           conv2d_474[0][0]                 \n",
      "                                                                 conv2d_478[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_479 (Conv2D)             (None, 100, 100, 64) 73792       concatenate_66[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_480 (Conv2D)             (None, 100, 100, 64) 36928       conv2d_479[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_481 (Conv2D)             (None, 100, 100, 64) 36928       conv2d_480[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "up_sampling2d_36 (UpSampling2D) (None, 200, 200, 64) 0           conv2d_481[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_482 (Conv2D)             (None, 200, 200, 16) 9232        up_sampling2d_36[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_483 (Conv2D)             (None, 200, 200, 16) 2320        conv2d_482[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_484 (Conv2D)             (None, 200, 200, 16) 2320        conv2d_483[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_485 (Conv2D)             (None, 200, 200, 16) 2320        conv2d_484[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_486 (Conv2D)             (None, 200, 200, 4)  580         conv2d_485[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "input_17 (InputLayer)           (None, 200, 200, 4)  0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_487 (Conv2D)             (None, 200, 200, 1)  37          conv2d_486[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_67 (Concatenate)    (None, 200, 200, 5)  0           input_17[0][0]                   \n",
      "                                                                 conv2d_487[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_488 (Conv2D)             (None, 200, 200, 16) 736         concatenate_67[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_489 (Conv2D)             (None, 200, 200, 16) 2320        conv2d_488[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_490 (Conv2D)             (None, 200, 200, 16) 2320        conv2d_489[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_85 (MaxPooling2D) (None, 100, 100, 16) 0           conv2d_490[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_68 (Concatenate)    (None, 100, 100, 96) 0           max_pooling2d_85[0][0]           \n",
      "                                                                 max_pooling2d_83[0][0]           \n",
      "                                                                 conv2d_481[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_491 (Conv2D)             (None, 100, 100, 16) 13840       concatenate_68[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_492 (Conv2D)             (None, 100, 100, 16) 2320        conv2d_491[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_493 (Conv2D)             (None, 100, 100, 16) 2320        conv2d_492[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_86 (MaxPooling2D) (None, 50, 50, 16)   0           conv2d_493[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_69 (Concatenate)    (None, 50, 50, 144)  0           max_pooling2d_86[0][0]           \n",
      "                                                                 max_pooling2d_84[0][0]           \n",
      "                                                                 conv2d_477[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_494 (Conv2D)             (None, 50, 50, 16)   20752       concatenate_69[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_495 (Conv2D)             (None, 50, 50, 16)   2320        conv2d_494[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_496 (Conv2D)             (None, 50, 50, 16)   2320        conv2d_495[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_87 (MaxPooling2D) (None, 25, 25, 16)   0           conv2d_496[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "reshape_5 (Reshape)             (None, 1, 10000)     0           max_pooling2d_87[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "dense_13 (Dense)                (None, 1, 2048)      20482048    reshape_5[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_80 (BatchNo (None, 1, 2048)      8192        dense_13[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dense_14 (Dense)                (None, 1, 256)       524544      batch_normalization_80[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_81 (BatchNo (None, 1, 256)       1024        dense_14[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dense_15 (Dense)                (None, 1, 7)         1799        batch_normalization_81[0][0]     \n",
      "==================================================================================================\n",
      "Total params: 21,467,968\n",
      "Trainable params: 21,463,072\n",
      "Non-trainable params: 4,896\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = MHNN_model()\n",
    "model.summary()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "mean_squared_error() missing 1 required positional argument: 'y_pred'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-51-a2ebf771d4c4>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;31m# loss_r2= tf.keras.losses.logcosh(tf_z, output2)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m \u001b[0mloss_r\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlosses\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mMSE\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtf_z\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m: mean_squared_error() missing 1 required positional argument: 'y_pred'"
     ]
    }
   ],
   "source": [
    "loss_c = tf.keras.losses.binary_crossentropy(tf_y_, output1_)\n",
    "loss_r = tf.keras.losses.MSE(output2, tf_z)\n",
    "loss_r2= tf.keras.losses.logcosh(tf_z, output2)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
