{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "F:\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "F:\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "F:\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "F:\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "F:\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "F:\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "F:\\Anaconda3\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "F:\\Anaconda3\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "F:\\Anaconda3\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "F:\\Anaconda3\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "F:\\Anaconda3\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "F:\\Anaconda3\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    }
   ],
   "source": [
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import sklearn\n",
    "import pandas as pd\n",
    "import os \n",
    "import sys\n",
    "import time\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".. _california_housing_dataset:\n",
      "\n",
      "California Housing dataset\n",
      "--------------------------\n",
      "\n",
      "**Data Set Characteristics:**\n",
      "\n",
      "    :Number of Instances: 20640\n",
      "\n",
      "    :Number of Attributes: 8 numeric, predictive attributes and the target\n",
      "\n",
      "    :Attribute Information:\n",
      "        - MedInc        median income in block\n",
      "        - HouseAge      median house age in block\n",
      "        - AveRooms      average number of rooms\n",
      "        - AveBedrms     average number of bedrooms\n",
      "        - Population    block population\n",
      "        - AveOccup      average house occupancy\n",
      "        - Latitude      house block latitude\n",
      "        - Longitude     house block longitude\n",
      "\n",
      "    :Missing Attribute Values: None\n",
      "\n",
      "This dataset was obtained from the StatLib repository.\n",
      "http://lib.stat.cmu.edu/datasets/\n",
      "\n",
      "The target variable is the median house value for California districts.\n",
      "\n",
      "This dataset was derived from the 1990 U.S. census, using one row per census\n",
      "block group. A block group is the smallest geographical unit for which the U.S.\n",
      "Census Bureau publishes sample data (a block group typically has a population\n",
      "of 600 to 3,000 people).\n",
      "\n",
      "It can be downloaded/loaded using the\n",
      ":func:`sklearn.datasets.fetch_california_housing` function.\n",
      "\n",
      ".. topic:: References\n",
      "\n",
      "    - Pace, R. Kelley and Ronald Barry, Sparse Spatial Autoregressions,\n",
      "      Statistics and Probability Letters, 33 (1997) 291-297\n",
      "\n",
      "(20640, 8)\n",
      "(20640,)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import fetch_california_housing\n",
    "\n",
    "housing = fetch_california_housing()\n",
    "print(housing.DESCR)\n",
    "print(housing.data.shape)\n",
    "print(housing.target.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(14448, 8) (14448,)\n",
      "(2064, 8) (2064,)\n",
      "(4128, 8) (4128,)\n"
     ]
    }
   ],
   "source": [
    "#分割数据集为：0.7训练集，0.1验证集，0.2测试集\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "x_train_all,x_test,y_train_all,y_test = train_test_split(\n",
    "        housing.data,\n",
    "        housing.target,\n",
    "        random_state = 7,\n",
    "        test_size=0.2)\n",
    "\n",
    "x_train,x_valid,y_train,y_valid = train_test_split(\n",
    "        x_train_all,\n",
    "        y_train_all,\n",
    "        random_state=11,\n",
    "        test_size=0.125)\n",
    "\n",
    "print(x_train.shape,y_train.shape)\n",
    "print(x_valid.shape,y_valid.shape)\n",
    "print(x_test.shape,y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#归一化操作  x = (s-u)/std   减去均值，除以方差\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "scaler = StandardScaler()\n",
    "\n",
    "\n",
    "x_train_scaler = scaler.fit_transform(x_train)\n",
    "x_valid_scaler = scaler.transform(x_valid)\n",
    "x_test_scaler = scaler.transform(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_dir = \"generate_csv\"\n",
    "if not os.path.exists(output_dir):\n",
    "    os.mkdir(output_dir)\n",
    "\n",
    "def save_to_csv(output_dir, data, name_prefix, header=None, n_parts=10):\n",
    "    path_format = os.path.join(output_dir, \"{}_{:02d}.csv\")\n",
    "    filenames = []\n",
    "    for file_idx, row_indices in enumerate(np.array_split(np.arange(len(data)),n_parts)):\n",
    "        part_csv = path_format.format(name_prefix,file_idx)\n",
    "        filenames.append(part_csv)\n",
    "        with open(part_csv, \"wt\", encoding=\"utf-8\") as f:\n",
    "            if header is not None:\n",
    "                f.write(header + \"\\n\")\n",
    "            for row_index in row_indices:\n",
    "                f.write(\".\".join([repr(col) for col in data[row_index]]))\n",
    "                f.write('\\n')\n",
    "                \n",
    "    return filenames\n",
    "\n",
    "\n",
    "\n",
    "# np.c_将x,y按行merge到一起\n",
    "train_data = np.c_[x_train_scaler,y_train]\n",
    "valid_data = np.c_[x_valid_scaler,y_valid]\n",
    "test_data = np.c_[x_test_scaler,y_test]\n",
    "header_cols = housing.feature_names + [\"MidianHouseValue\"]\n",
    "header_str = \",\".join(header_cols)\n",
    "\n",
    "\n",
    "train_filenames = save_to_csv(output_dir, train_data, \"train\", header_str, n_parts=20)\n",
    "valid_filenames = save_to_csv(output_dir, valid_data, \"valid\", header_str, n_parts=10)\n",
    "test_filenames = save_to_csv(output_dir, test_data, \"test\", header_str, n_parts=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(b'generate_csv\\\\train_07.csv', shape=(), dtype=string)\n",
      "tf.Tensor(b'generate_csv\\\\train_11.csv', shape=(), dtype=string)\n",
      "tf.Tensor(b'generate_csv\\\\train_06.csv', shape=(), dtype=string)\n",
      "tf.Tensor(b'generate_csv\\\\train_16.csv', shape=(), dtype=string)\n",
      "tf.Tensor(b'generate_csv\\\\train_18.csv', shape=(), dtype=string)\n",
      "tf.Tensor(b'generate_csv\\\\train_15.csv', shape=(), dtype=string)\n",
      "tf.Tensor(b'generate_csv\\\\train_02.csv', shape=(), dtype=string)\n",
      "tf.Tensor(b'generate_csv\\\\train_12.csv', shape=(), dtype=string)\n",
      "tf.Tensor(b'generate_csv\\\\train_17.csv', shape=(), dtype=string)\n",
      "tf.Tensor(b'generate_csv\\\\train_05.csv', shape=(), dtype=string)\n",
      "tf.Tensor(b'generate_csv\\\\train_10.csv', shape=(), dtype=string)\n",
      "tf.Tensor(b'generate_csv\\\\train_13.csv', shape=(), dtype=string)\n",
      "tf.Tensor(b'generate_csv\\\\train_04.csv', shape=(), dtype=string)\n",
      "tf.Tensor(b'generate_csv\\\\train_01.csv', shape=(), dtype=string)\n",
      "tf.Tensor(b'generate_csv\\\\train_14.csv', shape=(), dtype=string)\n",
      "tf.Tensor(b'generate_csv\\\\train_09.csv', shape=(), dtype=string)\n",
      "tf.Tensor(b'generate_csv\\\\train_08.csv', shape=(), dtype=string)\n",
      "tf.Tensor(b'generate_csv\\\\train_00.csv', shape=(), dtype=string)\n",
      "tf.Tensor(b'generate_csv\\\\train_03.csv', shape=(), dtype=string)\n",
      "tf.Tensor(b'generate_csv\\\\train_19.csv', shape=(), dtype=string)\n"
     ]
    }
   ],
   "source": [
    "# 将csv文件做成dataset\n",
    "# 1.filename ——> dataset\n",
    "# 2.read file ——>datasets——>merge\n",
    "# 3.parse csv\n",
    "\n",
    "#第一步\n",
    "filename_dataset = tf.data.Dataset.list_files(train_filenames)\n",
    "for filename in filename_dataset:\n",
    "    print(filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b'-0.5687498121582446.-0.6839500714571032.-0.5744204742578485.0.2012054449732681.1.2049227044431197.-0.05090126825905318.-0.6696956958191057.0.5659680258034845.1.405'\n",
      "b'0.9314946022148806.0.3463524390603567.0.0770311506543982.-0.19001211133248155.-0.16727471480020087.0.028420670665991438.-0.8102343545280389.0.7853385485962785.2.028'\n",
      "b'0.3691379802417763.0.26709839978978284.-0.3008978077378238.-0.1962179848021484.-0.078327997253277.-0.12176531847855512.-0.8477113301837532.0.5958821880025021.3.812'\n",
      "b'-0.4664841610183416.-0.763204110727677.-0.7674069999147513.-0.08599256244182751.1.0607081235660683.-0.08660197136701311.-0.6696956958191057.0.5559966384038143.2.021'\n",
      "b'-1.202138755658554.-0.28767987510423404.-0.14411769350093734.0.061946351535992025.-0.504063256968165.-0.06862474035776597.1.8037846974580825.-0.7602264983529142.0.781'\n",
      "b'1.3604567027317471.-1.0009662285393985.0.579532723789923.-0.2248450785493213.0.5503439481269227.0.02097144062348479.-0.7212265373457135.0.855138260393984.2.639'\n",
      "b'0.25271809444461374.-0.6046960321865293.0.08375088537617953.-0.06725675786801832.0.6919678479103742.-0.12208139572375766.-0.6509572079912486.0.5061397014054562.4.075'\n",
      "b'-0.004042856016961147.-1.0802202678099724.0.1079737464219163.-0.19001211133248155.-0.11459753256367314.-0.006420762332851602.0.9511835012905657.-0.6954124802550402.1.452'\n",
      "b'0.7490042114677805.0.5048605176015044.0.5082498859316498.-0.14690134772380742.-0.4358419881700389.0.07062845231575743.-0.838342086269823.0.7304959178980818.2.262'\n",
      "b'-0.7521281069244348.-0.763204110727677.-0.16842036428151244.-0.20269458429565976.-0.11459753256367314.0.19277772447559913.0.4592981958093075.0.06739865581987531.0.658'\n",
      "b'0.041188019308450466.1.3766549495778166.-0.2736214478345589.-0.343230095509256.-0.8814391362691915.-0.039954061742846034.0.6841600497435995.-1.2587958683365235.2.226'\n",
      "b'-0.238971813538557.-0.8424581499982509.0.2730180240473652.0.06884932312142991.-0.4177072205148408.0.030139339676044798.0.49209054950805825.0.0773700432195526.1.153'\n",
      "b'-0.585411069928229.-0.12917179656308633.-0.2779697840498122.-0.0788539659493314.-0.07141951433701108.0.16620308502437411.0.07047457338126502.0.15714114241692836.0.598'\n",
      "b'-0.9619346036705998.-0.5254419929159555.0.15462246516757425.0.328800154393684.-0.04033134121381438.-0.03808571339644134.2.768816820592745.-2.1711778154065273.0.711'\n",
      "b'-0.27882303510124645.-1.714252581974563.-0.15708348692194649.0.07358872165981856.0.9924868547679422.-0.11437541244211696.-0.6931188056039296.1.124365720185129.1.222'\n"
     ]
    }
   ],
   "source": [
    "# 第二步\n",
    "\n",
    "n_readers = 5\n",
    "dataset = filename_dataset.interleave(\n",
    "    lambda filename: tf.data.TextLineDataset(filename).skip(1), # 按行读取文件，忽略第一行的header\n",
    "    cycle_length = 5\n",
    ")\n",
    "\n",
    "for line in dataset.take(15):\n",
    "    print(line.numpy())\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[<tf.Tensor: id=130, shape=(), dtype=int32, numpy=1>, <tf.Tensor: id=131, shape=(), dtype=int32, numpy=2>, <tf.Tensor: id=132, shape=(), dtype=int32, numpy=3>, <tf.Tensor: id=133, shape=(), dtype=int32, numpy=4>, <tf.Tensor: id=134, shape=(), dtype=int32, numpy=5>]\n"
     ]
    }
   ],
   "source": [
    "# 第三步\n",
    "# tf.io.decde_csv(str,record_defaults)\n",
    "# 将csv解析为tensor\n",
    "\n",
    "sample_str = '1,2,3,4,5'\n",
    "record_defaults = [tf.constant(0,dtype=tf.int32)] * 5\n",
    "parsed_fields = tf.io.decode_csv(sample_str,record_defaults)\n",
    "print(parsed_fields)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
